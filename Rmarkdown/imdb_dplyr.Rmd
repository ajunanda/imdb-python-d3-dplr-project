---
title: "IMDB dplyr Data Munging Exercise"
author: "Robert Chang"
date: "November 15, 2014"
output: html_document
---

In pursuing IMBD project using the Python:R:d3.js stack, this section is devoted to practicing **dplyr** in R. I recently encountered a very helpful [dplyr documentation](http://rstudio-pubs-static.s3.amazonaws.com/11068_8bc42d6df61341b2bed45e9a9a3bf9f4.html), but never gotten a chance to play with it. The goal of this Rmarkdown document is learn by doing. Hopefully, by the end of the this document, there is no more ugly referencing like `df[df$col1 = ]$col2` in my R code.

### STEP 1: Load Data
Let's load dplyr first. The two dataset that we will be playing with are named `movie_data_basic.csv` and `movie_data_more.csv`. 

**Pro Tip**: We can `ls` our current working directory by either `dir()` or `list.files()`. `dir()` is arguably less verbose.

```{r}
#setwd("~/Desktop/imdb_project/data")
library(dplyr)

#list.files()
dir()
```

Ok, let's now load the data using the usual read.csv functiion (I didn't use read.table because read.csv usually works better with specific delimiters).

```{r}
df.basic = read.csv(file = "./../data/movie_data_basic.csv", sep = ",", header = FALSE)
#df.basic = read.csv(file = "movie_data.csv", sep = ",", header = FALSE)
colnames(df.basic) = c("name", "year", "genre", "runtime", "rating", "num_votes", "img_src")
head(df.basic, 5)

df.more = read.csv(file = "./../data/movie_data_more.csv", sep = ",", header = FALSE)
#df.more = read.csv(file = "movie_box_office.csv", sep = ",", header = FALSE)
colnames(df.more) = c("name", "revenue", "director", "actor1", "actor2", "actor3", "rated")
head(df.more, 5)
```

### STEP 2: Data Join

The reason that I have two separate dataset is because I wasn't able to find a page that contains ratings & revenue in the same url, and I had to scrape the data twice via two different ulrs. This demonstrates real world data munging, or my lack of python data scraping for that matter. Anyway, we can see that we will probably use the `name` column as the unique identifier for each row. According to [Hadley's github page](https://github.com/hadley/dplyr), different joins have been implemented in `dplyr`. In our case, let us just inner join df.basic and df.more using name, and here is the result:

```{r}
df.basic$name = tolower(df.basic$name)
df.more$name = tolower(df.more$name)
df.joined = inner_join(df.basic, df.more, by = 'name')
#write.table(df.joined, file = "d3_imdb_data.csv", quote = FALSE, sep = ",", row.names = FALSE, col.names = TRUE)
dim(df.joined)
head(df.joined)
```

**Pro Tip**: For consistency, notice that we lower the movie names before we do the join, and in the join statement, we use the `by` argument to specify the join key. 

From `dim`, we see that there are 3373 records in the joined dataset. Here is a point worth mentioning -- we actually grabbed the original datasets by two URL requests: one sorted by number of votes, and one by revenue. Although there might be a lot of good questions we can ask, the implicit sorting of the movies (due to scraping) might inadvertently biased our analysis.

For example, our analysis might biased toward those that have large revenues and high votes, so any observations based on averaging might be biased toward more popular movies.

#### This thing call tbl_df

**Pro Tip**: dplyr can work with data frames as is, but if you're dealing with large data, it's worthwhile to convert them to a tbl_df: this is a wrapper around a data frame that won't accidentally print a lot of data to the screen. Here is an illustration:

```{r}
df = tbl_df(df.joined)
df

# another way to look at it
df %>% glimpse()
```

It has some advantages and disadvantages. It will reformat the df nicely and it shows you basic information on the type of each column. However, it might choose to hide some of the columns for formatting purposes. 

**Pro Tip**: `glimpse()` makes it possible to see all the columns in a tbl, displaying as much data for each variable as can be fit on a single line.

### STEP 3: Basic Exploratory Analysis

There are several interesting questions that we can ask by examining df.joined. For example:

* Is there a particular genre that tends to get higher ratings, votes, or revenue?
* Is there a particular year where movies are doing great in terms of revenue?
* Is there a particular director or movie star that tend to have great hits?

Let's answer each question one by one, and from this exercise, we will learn how to use all the fun tools in `dplyr`, such as `filter`, `arrange`, `select`, `mutate`, `summarise`, `group_by`, `%>%` operation ...etc.

#### Is there a particular genre that is super popular?

Before we answer this question, let's just count how many movies each genre has as a warmup exercise. If I am using `plyr`, I would typically type:

```{r, eval = FALSE}
df.by.genre = ddply(df, .(genre), summarise, cnt = length(genre))
df.by.genre = df.by.genre[order(df.by.genre$cnt),]
df.by.genre
```

*Note: use `{r, eval = FALSE}` if you do not want to evaluate the block. The side effect is that there will be no output since we didn't execute any code blocks.

This is too verbose, and I think we can do much better with `dplyr`:

```{r, eval = FALSE}
df_grp = group_by(df, genre)
df_stats = summarise(df_grp, cnt = n())
df_ordered_stats = arrange(df_stats, desc(cnt))
```

**Pro Tip**: This is slightly better than `ddply`, since each logical operation is in its own line. However, it's still a bit too verbose, and we can further simply this code by using the `%>%` (pronouced pipe) symbol:

```{r}
df %>% group_by(genre) %>% summarise(cnt = n()) %>% arrange(desc(cnt))
```

This is our first clean `dplyr` statement -- look how succinct it is! I hope you can appreciate how chaining/piping made this series of data manipulation a lot more clearer. Looking at the list, it seems Comedy, Action, and Drama are the top 3 prolific genres.

Ok, back to our original question, is there a particular genre that is super popular? Let's answer this question by three specific ranking metrics -- we will rank the genres by average ratings, average revnue, and finally average number of votes.

```{r}
# By revenue
df %>% 
   group_by(genre) %>% 
   summarise(cnt = n(),
                 avg_rating = mean(rating),
                 avg_revenue = mean(revenue),
                 avg_votes = mean(num_votes)) %>%
  arrange(desc(avg_revenue)) %>%
  head(5)

# By average rating
df %>% 
   group_by(genre) %>% 
   summarise(cnt = n(),
                 avg_rating = mean(rating),
                 avg_revenue = mean(revenue),
                 avg_votes = mean(num_votes)) %>%
  arrange(desc(avg_rating))

# By average votes
df %>% 
   group_by(genre) %>% 
   summarise(cnt = n(),
                 avg_rating = mean(rating),
                 avg_revenue = mean(revenue),
                 avg_votes = mean(num_votes)) %>%
  arrange(desc(avg_votes))
```

*Pro Tip*: Sometimes to improve code readability, we would break up the chained commands into separate line, one for each logical operation. In these cases, make sure to put the pipe in the end of each intermediate line, so R knows that it needs to continue the evaluation for that code block.

Back to the analysis, it seems like `Sci-Fi` category get the most revenue. In terms of votes and ratings, the highest avg vote/rating genre is `Western`, but notice we only have 4 movies in that category, so this is probably more noise than an actual signal. I am curious which those 4 movies are, so let's look them up with `filter`:

```{r}
df %>% filter(genre == 'Western')
```

Wow, Clint Eastwood must be happy that his movies are there! Also, most of the Western movies are relatively old, except django unchained (2012). To improve the analysis, let's get rid of genres that have a small sample size issue (< 100 records):

```{r}
# By ratings
df %>%
   group_by(genre) %>% 
   summarise(cnt = n(),
                 avg_rating = mean(rating),
                 avg_revenue = mean(revenue),
                 avg_votes = mean(num_votes)) %>%
  filter(cnt >= 100) %>%
  arrange(desc(avg_rating))
# By votes
df %>%
   group_by(genre) %>% 
   summarise(cnt = n(),
                 avg_rating = mean(rating),
                 avg_revenue = mean(revenue),
                 avg_votes = mean(num_votes)) %>%
  filter(cnt >= 100) %>%
  arrange(desc(avg_votes))
```

With the new analysis, we see that Biography genre has the highest average rating, and Crime has the highest average votes. Cool, we've made some progress here, let's move on to the next question.

#### Is there a particular year in which the movie industry is making a lot of revenue? What is the over trend over time?

Is the movie indsutry making more money over the years, you might ask? Let's take a look:

```{r}
df.by.year = df %>%
   select(year, revenue) %>%
   group_by(year) %>%
   summarise(cnt = n(), avg_revenue = mean(revenue)) %>%
   arrange(desc(year))

library(ggplot2)
library(scales)
ggplot(df.by.year, aes(x = year, y = avg_revenue)) + geom_point() + geom_line() + xlab("year") + ylab("avg revenue") + scale_y_continuous(labels = comma) 
```

**Pro Tip**: For ggplot2, I love `library(scales)` for formatting the axes. To learn more, you can check out this [man page](http://docs.ggplot2.org/current/scale_continuous.html). 

Coming back to the analysis, it seems the revenue flunctuates a lot, maybe because we didn't have enough data points for earlier decades? We can easily check this by 

```{r}
df.by.year %>% tail(10)
```
and we can confirm that this claim is true. Alternatively, we can just plot year with cnt in df.by.year.

```{r}
df.by.year.gap = df.by.year %>% filter(year %% 5 == 0)
ggplot(df.by.year.gap, aes(x = factor(year), y = cnt)) + geom_bar(stat = "identity")
```

I only looked at the trend for cnt for every 5 years, otherwise the plot is not readable. With this visualization, it does seem like before 1990, the number of movies in our dataset is a lot smaller. So it's best if we focus on trend post-1990s.

```{r, eval = FALSE}
ggplot(df.by.year %>% filter(year >= 1990), aes(x = year, y = avg_revenue)) + geom_point() + geom_line() + xlab("year") + ylab("avg revenue") + scale_y_continuous(labels = comma)
```

Equivalently, we can write:

```{r}

df.by.year %>% filter(year >= 1990) %>% 
    ggplot(aes(x = year, y = avg_revenue)) + 
    geom_point() + 
    geom_line() + 
    xlab("year") + 
    ylab("avg revenue") + 
    scale_y_continuous(labels = comma)
```

**Pro Tip**: `%>%` is SUPER powerful, all it does is make $f(x,y)$ becomes `x %>% f(y)`, so you can use `%>%` on essentially any function, including `ggplot`, `head`, `tail`, `str` and anything that you use frequently in one line without storing intermediate data frames!

Coming back to the analysis, looks like the movie industry is doing very well since 1990s. It increase from 50M in the 1990s to 80M in the 2010s! Up and to the Right! 

#### Is there a particular director or movie star that tend to have great hits?

Let us start with directors, since there is only director has only one column, as opposed to 3 actor columns. We all know it's going to be James Cameron, but let's double check:

```{r}
df.by.director = df %>% 
        group_by(director) %>%
        summarise(cnt_uu = n_distinct(name),
                  cnt = n(),
                  avg_revenue = mean(revenue)) %>%
        arrange(desc(avg_revenue))

df.by.director %>% head(10) # top 10 money making directors
```

**Pro Tip**: Urgh!!! when counting distinct, use `n_distinct` intead of `count_distinct`, which was the wrong suggestion in the vignette.

Hmm..., who the heck is Joss Whedon, and what movies did he make?

```{r}
df.joined %>% filter(director == 'Joss Whedon')
```

and wait, where's James cameron? let's search for him:

```{r}
df.joined %>% filter(director == 'James Cameron')
```

I think I see the problem now, James had made some bad movies in the past ('True lies`?), so taking the average might not be fair. Let's take max (or median):

```{r}
df %>% 
  group_by(director) %>%
  summarise(cnt_uu = n_distinct(name),
            cnt = n(),
            max_revenue = max(revenue),
            median_revenue = median(revenue)) %>%
  arrange(desc(max_revenue))
```

Ok, if we look at it by max revenue, James Cameron is still number 1. If we rank by median revenue, the list probably would be different. Be aware of what questions you are trying to ask, and see what statistic is appropriate for your context!

Now, to see which movie star tend to associate with high box office revenue, we will need to do a bit more work. The reason is that our data schema store 3 actors for each movie, so to do what we need, we need to melt the dataset.

```{r}
library(reshape2)
# dataset pre-melt
df %>% head(3) %>% select(name, actor1, actor2, actor3)

# dataset melted
df %>% head(3) %>%
  select(name, actor1, actor2, actor3, revenue) %>%
  melt(id = c("name", "revenue")) %>% 
  select(name, value, revenue)
```

**Pro Tip**: You can take a look at Hadley's [documentation](http://had.co.nz/reshape/introduction.pdf) or reference this [quick-R page](http://www.statmethods.net/management/reshape.html).

The above code is just to demonstrate what melt does. It essentially break each record into 3 records, where each actor involved in the same movie has its own row. I think this has to do with the concept of pivot table. Now we are ready to proceed with the analysis:

```{r}
df.by.actor = df %>%
  select(name, actor1, actor2, actor3, revenue) %>%
  melt(id = c("name", "revenue")) %>% 
  select(name, value, revenue)

colnames(df.by.actor) = c("movie", "actor", "revenue")

df.by.actor %>% 
  group_by(actor) %>%
  summarise(cnt = n(),
            count_uu = n_distinct(movie),
            avg_revenue = mean(revenue)) %>%
  filter(count_uu > 10) %>%
  arrange(desc(avg_revenue)) %>%
  head(50)
```

*Note: cnt and count_uu should be the same, but I think there is a bug in the IMBD database, we have duplicate movies with wrong year field. Examples: `the avengers` & `the nutty professor`.

Wow, Shia LaBeouf is the number 1 actor, too bad he dropped out from Hollywood! This is a very cool dataset to look at, and we might ask more interesting questions. For example, of the actors who had 10+ movies, which one has the highest average box office numbers compared to the overall box office average, across all actors.

```{r}
df.by.actor %>%
  mutate(global_avg = mean(revenue)) %>%
  group_by(actor) %>%
  summarize(cnt = n(),
            global_avg = first(global_avg),
            avg_revenue = mean(revenue)) %>%
  mutate(inflation_factor = avg_revenue / global_avg) %>%
  arrange(desc(inflation_factor)) %>% head()
```

*Note: Again, the intro to plyr vignette is wrong, use `first` instead of `first_value`, `last` instead of `last_value`. Here is an [stack overflow example](http://stackoverflow.com/questions/25184965/r-dplyr-how-to-use-last-first-nth-correct-in-a-sql-database).

**Pro Tip**: Here, we demonstrate how to use `mutate` when we need to add a new column. It's still kind of stupid that I had to use `first` to extract that value though. Not sure what's a more efficient way of doing it.

### Some more Advanced Features of dplyr

We have demonstrated the basics of dplyr, and by going through this exercise, hopefully you have developed a good habit of using this new approach to make your code look cleaner, but this is by no means the end. Here are additional resources:

* We didn't talk about how if you `group by` multiple variables, each `summarise` will unpeel one variable at a time.
* We didn't talk about windows function or working with databases.
* We didn't talk about the new stuffs in dplyr 0.2 or 0.3. See [0.2 annoucement](http://blog.rstudio.org/2014/05/21/dplyr-0-2/) and [0.3 annoucement](http://blog.rstudio.org/2014/10/13/dplyr-0-3-2/).

* We didn't talk about tidyr

#### Here is an example of using tidyr (courtesy of Joshua Lande)
```{r}
library(dplyr)
library(tidyr)

data.frame(p=seq(0.1,0.9,length=1000)) %>%
  rowwise() %>%
  mutate(relative=power.prop.test(p1=p, p2=p*(if (p < 0.5) 1.01 else 0.99), 
                                  power=0.8, sig.level=0.05)[[1]],
         absolute=power.prop.test(p1=p, p2=p+(if (p < 0.5) 0.01 else - 0.01), 
                                  power=0.8, sig.level=0.05)[[1]]) %>%
  gather(type, n, relative:absolute) %>%
  ggplot(aes(x=p, y=n, color=type)) +
  geom_line() + 
  ylim(0,500000) +
  xlab('Baseline Probability') +
  ylab('Number of Users') +
  ggtitle('Number of Users needed to see a 1% Change')
```

The new concept is `rowwise` and `gather`. see [rowwise](http://www.rdocumentation.org/packages/dplyr/functions/rowwise) and [intro to tidyr](http://blog.rstudio.org/2014/07/22/introducing-tidyr/).


